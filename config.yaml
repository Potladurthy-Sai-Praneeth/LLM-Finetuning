environment:
  HF_HUB_ENABLE_HF_TRANSFER : "1" 

model:
  # BASE_MODEL_ID : "google/gemma-3-4b-pt"
  # CHAT_MODEL_ID : "google/gemma-3-4b-it"
  # MAX_SEQ_LENGTH : 512
  # IMG_SIZE : 448

  BASE_MODEL_ID : "google/paligemma-3b-pt-224"  
  CHAT_MODEL_ID : "google/paligemma-3b-pt-224"  
  MAX_SEQ_LENGTH : 512
  IMG_SIZE : 224  

dataset:
  DATASET_ID : "mmoukouba/MedPix-Grouped-QA"
  NUM_SAMPLES : 500

training:
  OUTPUT_DIR : "gemma-medical"
  BATCH_SIZE : 1
  GRADIENT_ACCUMULATION_STEPS : 4
  LEARNING_RATE : 2e-4
  WEIGHT_DECAY : 0.0
  NUM_TRAIN_EPOCHS : 2
  LR_WARMUP_STEPS : 50
  MAX_GRAD_NORM : 0.3
  SAVE_TOTAL_LIMIT : 3
  SAVE_STEPS : 500
  EVAL_STEPS : 500
  LOGGING_STEPS : 10
  SEED : 42

lora:
  LORA_R : 8
  LORA_ALPHA : 16
  LORA_DROPOUT : 0.05
  TASK_TYPE : "CAUSAL_LM"
  # MODULES_TO_SAVE : ["lm_head", "embed_tokens"]
  MODULES_TO_SAVE : []
  # TARGET_MODULES : ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  TARGET_MODULES : ["q_proj", "k_proj", "v_proj"]

fsdp:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_forward_prefetch: false
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_cpu_ram_efficient_loading: false
  fsdp_sync_module_states: true
  fsdp_use_orig_params: false




# All unique module types in the Gemma3 VLM model:
#   - AvgPool2d
#   - Conv2d
#   - Embedding
#   - Gemma3Attention
#   - Gemma3DecoderLayer
#   - Gemma3ForConditionalGeneration
#   - Gemma3MLP
#   - Gemma3Model
#   - Gemma3MultiModalProjector
#   - Gemma3RMSNorm
#   - Gemma3RotaryEmbedding
#   - Gemma3TextModel
#   - Gemma3TextScaledWordEmbedding
#   - LayerNorm
#   - Linear
#   - Linear4bit
#   - ModuleList
#   - PytorchGELUTanh
#   - SiglipAttention
#   - SiglipEncoder
#   - SiglipEncoderLayer
#   - SiglipMLP
#   - SiglipVisionEmbeddings
#   - SiglipVisionModel
#   - SiglipVisionTransformer
