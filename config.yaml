environment:
  HF_HUB_ENABLE_HF_TRANSFER : "1"
  # Remove CUDA_VISIBLE_DEVICES to let the script auto-detect all GPUs
  # CUDA_VISIBLE_DEVICES : "0,1"  

model:
  BASE_MODEL_ID : "google/gemma-3-4b-pt"
  CHAT_MODEL_ID : "google/gemma-3-4b-it"
  MAX_SEQ_LENGTH : 512

dataset:
  DATASET_ID : "mmoukouba/MedPix-Grouped-QA"

training:
  OUTPUT_DIR : "gemma-medical"
  BATCH_SIZE : 2
  GRADIENT_ACCUMULATION_STEPS : 2
  LEARNING_RATE : 2e-4
  WEIGHT_DECAY : 0.0
  NUM_TRAIN_EPOCHS : 2
  LR_WARMUP_STEPS : 100
  MAX_GRAD_NORM : 0.3
  SAVE_TOTAL_LIMIT : 3
  SAVE_STEPS : 500
  EVAL_STEPS : 500
  LOGGING_STEPS : 10
  SEED : 42
  # MERGED_MODEL_DIR : "gemma-medical-merged"
  # USE_FSDP : true  # Set to false to disable FSDP for single GPU

lora:
  LORA_R : 8
  LORA_ALPHA : 16
  LORA_DROPOUT : 0.05
  TASK_TYPE : "CAUSAL_LM"
  MODULES_TO_SAVE : ["lm_head", "embed_tokens"]
  TARGET_MODULES : ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

fsdp:
  auto_wrap_policy: 'TRANSFORMER_BASED_WRAP'
  backward_prefetch: 'BACKWARD_PRE'
  cpu_ram_efficient_loading: True
  forward_prefetch: False
  offload_params: True
  sharding_strategy: 'FULL_SHARD'  
  state_dict_type: 'SHARDED_STATE_DICT'
  sync_module_states: True
  use_orig_params: True
  offload_optimizer: True 
  activation_offload: True
  # transformer_layer_cls_to_wrap: ['Gemma3DecoderLayer']




# All unique module types in the Gemma3 VLM model:
#   - AvgPool2d
#   - Conv2d
#   - Embedding
#   - Gemma3Attention
#   - Gemma3DecoderLayer
#   - Gemma3ForConditionalGeneration
#   - Gemma3MLP
#   - Gemma3Model
#   - Gemma3MultiModalProjector
#   - Gemma3RMSNorm
#   - Gemma3RotaryEmbedding
#   - Gemma3TextModel
#   - Gemma3TextScaledWordEmbedding
#   - LayerNorm
#   - Linear
#   - Linear4bit
#   - ModuleList
#   - PytorchGELUTanh
#   - SiglipAttention
#   - SiglipEncoder
#   - SiglipEncoderLayer
#   - SiglipMLP
#   - SiglipVisionEmbeddings
#   - SiglipVisionModel
#   - SiglipVisionTransformer
