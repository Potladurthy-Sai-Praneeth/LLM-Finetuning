environment:
  HF_HUB_ENABLE_HF_TRANSFER : "1"
  # Remove CUDA_VISIBLE_DEVICES to let the script auto-detect all GPUs
  # CUDA_VISIBLE_DEVICES : "0,1"  

model:
  BASE_MODEL_ID : "google/gemma-3-4b-pt"
  CHAT_MODEL_ID : "google/gemma-3-4b-it"
  MAX_SEQ_LENGTH : 512

dataset:
  DATASET_ID : "mmoukouba/MedPix-Grouped-QA"

training:
  OUTPUT_DIR : "gemma-medical"
  BATCH_SIZE : 2
  GRADIENT_ACCUMULATION_STEPS : 2
  LEARNING_RATE : 2e-4
  WEIGHT_DECAY : 0.0
  NUM_TRAIN_EPOCHS : 2
  LR_WARMUP_STEPS : 100
  MAX_GRAD_NORM : 0.3
  SAVE_TOTAL_LIMIT : 3
  SAVE_STEPS : 500
  EVAL_STEPS : 500
  LOGGING_STEPS : 10
  SEED : 42
  # MERGED_MODEL_DIR : "gemma-medical-merged"
  # USE_FSDP : true  # Set to false to disable FSDP for single GPU

lora:
  LORA_R : 8
  LORA_ALPHA : 16
  LORA_DROPOUT : 0.05
  TASK_TYPE : "CAUSAL_LM"
  MODULES_TO_SAVE : ["lm_head", "embed_tokens"]
  TARGET_MODULES : ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

fsdp:
  fsdp_auto_wrap_policy: 'TRANSFORMER_BASED_WRAP'
  fsdp_backward_prefetch: 'BACKWARD_PRE'
  fsdp_cpu_ram_efficient_loading: True
  fsdp_forward_prefetch: False
  fsdp_offload_params: True
  fsdp_sharding_strategy: 'FULL_SHARD'  
  fsdp_state_dict_type: 'SHARDED_STATE_DICT'
  fsdp_sync_module_states: True
  fsdp_use_orig_params: False
  fsdp_offload_optimizer: True 
  fsdp_activation_offload: True
  fsdp_transformer_layer_cls_to_wrap: ['Gemma3DecoderLayer']
