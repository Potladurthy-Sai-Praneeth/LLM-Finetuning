environment:
  HF_HUB_ENABLE_HF_TRANSFER : "1" 

model:
  BASE_MODEL_ID : "google/gemma-3-4b-pt"
  CHAT_MODEL_ID : "google/gemma-3-4b-it"
  MAX_SEQ_LENGTH : 256
  IMG_SIZE : 224

dataset:
  DATASET_ID : "mmoukouba/MedPix-Grouped-QA"

training:
  OUTPUT_DIR : "gemma-medical"
  BATCH_SIZE : 1
  GRADIENT_ACCUMULATION_STEPS : 4
  LEARNING_RATE : 2e-4
  WEIGHT_DECAY : 0.0
  NUM_TRAIN_EPOCHS : 2
  LR_WARMUP_STEPS : 50
  MAX_GRAD_NORM : 0.3
  SAVE_TOTAL_LIMIT : 3
  SAVE_STEPS : 500
  EVAL_STEPS : 500
  LOGGING_STEPS : 10
  SEED : 42

lora:
  LORA_R : 4
  LORA_ALPHA : 8
  LORA_DROPOUT : 0.05
  TASK_TYPE : "CAUSAL_LM"
  MODULES_TO_SAVE : ["lm_head", "embed_tokens"]
  TARGET_MODULES : ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

fsdp:
  fsdp_auto_wrap_policy: 'TRANSFORMER_BASED_WRAP'
  fsdp_backward_prefetch: 'BACKWARD_PRE'
  fsdp_cpu_ram_efficient_loading: True
  fsdp_forward_prefetch: False
  fsdp_offload_params: True
  fsdp_sharding_strategy: 'FULL_SHARD'  
  fsdp_state_dict_type: 'SHARDED_STATE_DICT'
  fsdp_sync_module_states: True
  fsdp_use_orig_params: True
  fsdp_offload_optimizer: True 
  fsdp_activation_offload: True




# All unique module types in the Gemma3 VLM model:
#   - AvgPool2d
#   - Conv2d
#   - Embedding
#   - Gemma3Attention
#   - Gemma3DecoderLayer
#   - Gemma3ForConditionalGeneration
#   - Gemma3MLP
#   - Gemma3Model
#   - Gemma3MultiModalProjector
#   - Gemma3RMSNorm
#   - Gemma3RotaryEmbedding
#   - Gemma3TextModel
#   - Gemma3TextScaledWordEmbedding
#   - LayerNorm
#   - Linear
#   - Linear4bit
#   - ModuleList
#   - PytorchGELUTanh
#   - SiglipAttention
#   - SiglipEncoder
#   - SiglipEncoderLayer
#   - SiglipMLP
#   - SiglipVisionEmbeddings
#   - SiglipVisionModel
#   - SiglipVisionTransformer
